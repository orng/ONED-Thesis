\chapter{Discussion}
\label{chapter:discussion}

\Crefrange{fig:setPlot1}{fig:setPlot4} show that over time, at least to begin with, the amount of new words found decreases while at the same time the amount of new pairs appears to increase. If we are more interested in new pairs than new words then it might therefore be a good idea to pre-train the system on some set of articles so as to skip past the period in which we mostly find new words.

The results of our experiments support our theory that TF-IDF is a good filter to use if we want to decrease the output size, but that filtering should be done on the output rather than the input.

\Cref{lst:config1,lst:config5,lst:config6} all have \lstinline{(question, bp), (spill, question), (spill, answer)} etc. among the first pairs of output, which capture the essence of the original article. Overall, the results presented in the previous chapter indicate that minimal-new sets can indeed be good indicators of new events or at least novelty within articles. However, simply checking for the existence of minimal new sets containing only one or two elements is not sufficient for determining if an article contains a new event or not and further work has to be done to develop a method to quantify the enumerations to calculate the probability of them relating to a new event.

Although it could be argued that our system could be useful for humans to perform tool-assisted New Event Detection where users scan through a small number of minimal new sets for incoming articles and are as such able to process the incoming article faster than they would by reading through them, the system still requires manual labour. In, fact since it is not yet able to perform automatic Event Detection it is not really an ONED-system.

One thing that is worth highlighting is the good performance of the sub-bag approach whic, as indicated by \cref{tab:benchmark1,tab:benchmark2}, achieves good results with a significant increase in performance. It should be noted for these benchmarks that each result was written to disk after being calculated. This might be partially responsible for the unfiltered configurations performing worse than the filtered ones. In any case, combining filtered and unfiltered configurations and grouping only on whether or not configurations use sub-bags as in \cref{tab:benchmark2} still points to sub-bags being 6.7 times faster. This is perhaps unsurprising since for articles with very different sub-bags the number of candidate sets to examine is smaller and checking if candidate sets are subsets of sub-bags is also slightly faster due to sub-bags being smaller. An additional potential benefit of the sub-bags approach is that each sub-bag could be processed in parallel and therefore the runtime could be improved even further, especially in cases when long articles are processed.

\section{Comparisons with other approaches}
Since we have not yet developed a true ONED system, comparisons with other methods becomes troublesome. Had we developed such a system we could have tested our approach on any of the TDT datasets such as TDT5 and compared the results with the results of other approaches. If we assume that the outputs of our approach could be quantified effectively we can compare the time complexity of our approach to other existing approaches.

As discussed in chapter~\ref{chapter:background} Wurzer, Lavrenko and Osborne~\cite{wurzer2015kterm} present an ONED system based on the proportion of subsets up to size $k$, for some small integer $k$, which have not been previously stored. Through the use of a Bloom-filter they are able to operate in constant time per input set in relation to the number of seen articles as well as in constant memory. In their tests they used $k=3$ and achieved impressive results. However, since they do not only consider minimal new sets they always have to find all possible subsets of size $k$ of the incoming bag $B_{n}$ whereas we might achieve faster times, especially for larger values of $k$. The base approach is bounded by \BigO{|B_{n}|2^{k}/k!} while their approach is bounded by \BigO{|B_{n}|} and since $\lim_{x \to \infty}2^{k}/k!=0$ our approach is better suited for larger values of $k$. 

Like Wurzer, Lavrenko and Osborne~\cite{wurzer2015kterm} and unlike all the other approaches discussed in chapter~\ref{chapter:background} ours is not time-bound by the number of processed articles and therefore does not slow down over time. However, unlike the Wurzer et al. the memory consumption of our approach grows with the number of processed articles as our approach requires storing all processed bags. 

One weakness of our current enumeration algorithm is events which are repeated or events which occur as history repeats itself, e.g. someone is shot in Times Square today and then someone else is shot during similar conditions some time later. This later event is a separate and new event, distinct from the first, but although would likely include new information such as other names etc., it is also possible that it would not result in any minimal new sets since they would have been introduced by the first shooting.

Since our enumeration algorithm builds up a collection of minimal new sets one can not easily remove old enumerations without sacrificing accuracy since the elements which were minimal-new in the set that is to be removed might have reappeared in a later set and the removal of the first set would therefore require the later set to be update. Because we do not know what sets require updates we would have to traverse all the stored bags which arrived after the set we are removing until we either run out of bags to check or minimal new sets found in the removed enumeration.

Others have developed such systems which forget seen articles over time. Yang, Pierce and Carbonell~\cite{yang1998study}, for instance, use a sliding window which specifies which previously seen stories they compare incoming stories to. Furthermore they explore weight decay where older stories within the window are given less weight.

\subsection{Future Work}
We believe that the results indicate that an effective fully automatic ONED system could be built using our approach of enumerating minimal new sets but different ways for quantifying the enumeration have to be studied. Possible quantification methods might include combining our approach with named entity extraction and using the number of minimal new sets containing named entities as indicators of new events. Another possible approach would be to incorporate the previously discussed technique used by Wurzer, Lavrenko and Osborne~\cite{wurzer2015kterm} and use the ratio of the amount of minimal new sets of size less than some integer $k$ over the amount of possible subsets of size less than $k$. We might be able to achieve better accuracy since we never discard any found minimal-new sets while they flip random bits in the Bloom-filter in order to decrease the probability of false positives.

It might also be possible to improve upon the filtering methods presented in this article through smarter filtering. Instead of generating all candidate sets it might be worthwhile to examine only certain words, for instance named entities or words with high TF-IDF scores and only include those as candidate sets when enumerating.

As we discussed earlier, not being able to forget stories over time might be a drawback. This may be a drawback and possible future work would be to modify the enumeration algorithm in order to allow for that and examine if it improves results at all.

Last but not least we would like to examine the feasibility of using minimal new sets to detect trending topics. If a word or group of words appear in many minimal new sets from articles in close succession this might be an indication that they belong to a trending topic.

\chapter{Background}
\label{chapter:background}

The task of detecting new events within a stream of documents can be separated into two camps: In \emph{Retrospective Event Detection} focuses on discovering previously unidentified from a finite collection of article[TODO: ref yang: p28-yang]. \emp{On-line New Event Detection (ONED), which is sometimes known as either simply \emph{New Event Detection} (NED) or \emph{First Story Detection} (FSD), instead deals with the problem as trying to identify new events in real-time as soon as they are arrive from live news feeds.

On-line New Event Detection has seen much development in the past 15 years as it has been one of the topics covered by the \emph{Topic Detection and Tracking}(TDT) research program[TODO:missing citation]. In this chapter we will present a brief overview of techniques used for ONED by other researchers within the field.

\section{What are events?}
The concept of events in daily speech can be somewhat ambiguous [TODO: cite papka 1999] and in order for us to reason about events it is important that we specify what we mean. "The United States invade Vietnam" could be considered an event but at the same time the whole Vietnam War could be considered an event. We use TDT's definition of events: An event is ``a particular thing that happens at a specific time and place, along with all necessary preconditions and unavoidable consequences''. [TODO: cite https://catalog.ldc.upenn.edu/docs/LDC2006T19/TDT2004V1.2.pdf]

\section{Related works}
A classical approach to ONED is to represent documents as term vectors where each term within a document is weighted using some metric, typically TF-IDF (term frequency-inverse document frequency)[TODO: citation] which are then compared in some way to the previously seen documents.

Allan, Papka and Lavrenko [TODO: cite 10-1.1.24..-allan] use a single-pass clustering technique where feature extraction and selection techniques are used to build query representations of all stories. They then compare any incoming document to the previously seen queries and flagging the document as new if it's comparison score exceeds a threshold which is calculated for each document.

Yang, Pierce and Carbonell [TODO: cite p28-yang] also use a single-pass clustering algorithm and incremental IDF for TF-IDF weighting: IDF is first trained on a dataset an then updated for each document. In order to increase efficiency and due to the temporal nature of events they use a sliding time-window to only compare incoming documents to stories within the time window. Additionally they also present results of applying decay weighting where documents further away in time are marked as less important.

Brants, Chen and Farahat [TODO: cite p.330-brants] use a source-specific TF-IDF model where the enews stream is comprised out of multiple sources and certain words are more common depending on the source. Each incoming document is weighted and compared to previously seen documents using either Hellinger- or cosine-distance but normalized by subtracting the average distance of current document to all other documents. [TODO: more stuff that they do]

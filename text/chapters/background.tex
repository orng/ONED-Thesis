\chapter{Background}
\label{chapter:background}

The task of detecting new events within a stream of documents can be separated into two camps, \emph{Retrospective Event Detection} and \emph{On-line New Event Detection}. Retrospective Event Detection focuses on discovering previously unidentified events from a finite collection of articles~\cite{yang1998study}. On-line New Event Detection (ONED), which is sometimes known as either simply \emph{New Event Detection} (NED) or \emph{First Story Detection} (FSD), instead tries to identify new events in real-time as soon as they arrive from live news feeds.

On-line New Event Detection has seen much development in the past 15 years as it has been one of the topics covered by the \emph{Topic Detection and Tracking} (TDT) research program~\cite{tdt2004annotation}. In this chapter we will present a brief overview of the techniques used for ONED by other researchers within the field.

\section{What are events?}
The concept of events in daily speech can be somewhat ambiguous~\cite{papka1999online} and in order for us to reason about events it is important that we specify what we mean. "The United States invade Vietnam" could be considered an event but at the same time the whole Vietnam War could be considered an event. We use TDT's definition of events: An event is ``a particular thing that happens at a specific time and place, along with all necessary preconditions and unavoidable consequences''~\cite{tdt2004annotation}. 
A new news-story does therefore not necessarily report a new event. For instance a news story reporting about a natural disaster would contain a new event, whereas consecutive articles detailing the extent of the damage caused and the particulars of the rebuilding efforts that follow, would not. In ONED we want to be able to detect only those stories which contain events, and in particular, only new events that have not been reported before.

\section{Related works}
A common approach to ONED is to represent documents as term vectors where each term within a document is weighted using some metric, typically TF-IDF (term frequency-inverse document frequency), which are then compared in some way to the vector representations of previously seen documents. Since comparing incoming stories to all previous stories is slow, organizing stories into clusters and comparing incoming stories to the clusters is a popular step employed by many. Often, the stories within the best fitting clusters would then be compared to the incoming story.

Papka, Allan and Lavrenko~\cite{papka1998online} use a single-pass clustering technique where feature extraction and selection techniques are used to build query representations of all stories. They then compare any incoming document to the previously seen queries and flag the document as new if its comparison score exceeds a threshold which is calculated for each document.

Yang, Pierce and Carbonell~\cite{yang1998study} also use a single-pass clustering algorithm and incremental IDF for TF-IDF weighting, IDF is first trained on a dataset an then updated for each document. In order to increase efficiency and due to the temporal nature of events they use a sliding time-window to only compare incoming documents to stories within the time window. Additionally they also examine decay weighting where documents further away in time are marked as less important.

Brants, Chen and Farahat~\cite{brants2003system} use a source-specific TF-IDF model where the news stream is comprised of multiple sources and certain words are more common depending on the source. Each incoming document is weighted and compared to previously seen documents using either Hellinger- or cosine-distance but normalized by subtracting the average distance of the current document to all other documents. 

Another popular approach is to represent documents using named entities, the idea being that events can be summarized  by ``what'', ``where'', ``when'' and ``how'' as well as other similar properties. 

Yang et al.~\cite{yang2002topic} use a supervised learning algorithm to classify documents in an on-line document stream into pre-defined topic categories. Weighting is done using named entities. They then perform topic specific stopword removal and topic sensitive feature weighting.

Kumaran and Allan~\cite{kumaran2005using} use named entities in addition to term vectors with incremental TF-IDF weighting as well as topic terms and compute the cosine similarities of each of those features to previously seen documents. A Support Vector Machine (SVM) classifier is then trained on each of those three features.

Zhang, Li and We~\cite{zhang2007new} create a tree of clustered stories from the stories they process which allows them to quickly find the stories which most closely resemble the incoming story by comparison. In addition they present two new approaches to term weighting. The first approach takes into account the frequency of words in the previously identified clusters, a word that is very common in one cluster but not common in others is deemed important for that cluster while a word that is common in all clusters is determined to be of less importance. The second weighing approach classifies various named entities into different classes and tries to determine what type of words are important for different story topics. For instance, in their testing they found that locations were of utmost importance for stories relating to natural disasters.

Petrovic, Osborne and Lavrenko~\cite{petrovic2012using} paraphrase incoming articles and use locality-sensitive hashing for comparisons. By paraphrasing they can for instance determine that ``die'' and ``kick the bucket'' hold the same meaning and achieve good results.

Wurzer, Lavrenko and Osborne~\cite{wurzer2015kterm} present an algorithm for ONED which operates in constant time/space with regards to the number of processed articles. Their approach is based on the same intuition as ours and revolves around identifying new small subsets of words from the words that make up an article. While our method focuses on the subsets themselves, their approach considers the amount of such new subsets identified. Particularly, they perform ONED by generating all subsets of up to $k$ terms for incoming documents and estimating novelty as the fraction of such sets that have not appeared before. 
For this purpose they store all these subsets using a Bloom filter with a 32-bit hashing function. For the Bloom filter they use a fixed-length bit array. This allows their system to perform lookups and updates in constant time as well as constant space. To limit the amount of false positives reported by Bloom-filter lookups they zero out a random bit each time the proportion of non-zero bits exceeds a given threshold. This of course means that the system forgets some of the sets of words that it has seen.

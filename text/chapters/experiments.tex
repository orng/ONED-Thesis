\chapter{Experiments}
\label{chapter:experiments}
TODO: explain in detail out implementations, approaches tested, how we tested and on what datasets, how the datasets were obtained etc.

We implemented the concepts discussed in chapter~\ref{chapter:method} using the Python (TODO: CITATION) programming language. We then used these implementations to test the effectiveness of the various methods on several different datasets. In this chapter we discuss our implementation in some detail as well as how the tests were carried out along with what data was used for testing.

\section{Implementation}
For our implementation we chose Python because it supports many high-level concepts such as lamda functions, list comprehensions etc. as well as for its extensive standard library and excellent availability of good third-party libraries such as the Natural Language Tool Kit (NLTK) (TODO: CITATION).

\subsection{Pre-processing}
The NLTK provided us with most of the parts needed for preprocessing: we used the built-in word-tokenizer for splitting whole-string articles into lists of words,  stopwords from the English corpus for stopword-removal and the porter2 stemmer for fast and simple stemming of words. Python's string module provided us with a list of punctuation characters which we used to remove punctuation. Our pre-processing step first removed any punctuation character, then tokenized the input-string, removed all the stopwords then stemmed each of the tokens. (TODO: CITATIONS? PORTER2, nltk)

\subsection{Enumeration}
We have implemented the algorithm described in section~\label{method:enumeration}. Our implementation uses sets of strings (or Python's frozensets in cases where the sets had to be hashed) to represent the bags of words to enumerate, a list of bags (list of sets of strings) to store previously seen bags and a dictionary (hashmap) with frozensets as keys and integers as values for mapping previously seen minimal new sets to the number of the bag in which they first appeared.

\section{Tests}
The implementations were tested on numerous different articles obtained by crawling various sources for on-line news. The crawled articles were stored along with their dates of publication so that they could later be consumed in the same order that they would have arrived should they have been processed by an on-line news monitoring system. For each digested article the original article was printed along with the output of the enumeration, which we then examined by hand. In order to determine if an approach was successful we focused on examining whether the output included minimal new sets which captured the main points of the input article.

Initial tests focused on testing the base method on short articles that all related to some common theme as an attempt to decrease the amount of false positives. The goal was to get an idea of the performance of the base idea on datasets on which we theorized that it would perform well. Timelines which detail major events of some era such as Barack Obama's years of presidency can be regarded as very dense articles which almost all report on new events and as such might provide easy insight into the performance of our method. Aside from timelines, our initial test also included sets of article summaries crawled from various news websites. Additionally, we tested our approach on a chronological account of the main events of the second world war, where each paragraph was treated as separate article. By focusing on short articles we hoped to decrease the overall number of unique words used within the dataset, thus decreasing the number of false positives as well as simplifying the work of manually validating the output of our tests.

Subsequent tests focused on larger sets of more diverse topics and longer articles, more closely imitating real-world usage of ONED algorithms.

\chapter{Method}
\label{chapter:method}
On-line new event detection is hard~\cite{allan2000hard}. In this chapter we present a new method which aims to perform ONED, based around the concept of \emph{minimal new pairs}. We explain the rationale behind the idea, give a detailed explanation of the base algorithm and discuss some of the approaches we have examined for expanding upon the base idea.


\section{The basic idea}
\label{section:idea}
Damaschke~\cite{damaschke2015pairs} observed that for new articles, the existence of previously unseen small sets of words could be good indicators for new events and presented an efficient algorithm for finding such sets. This is similar to the approach used by Wurzer et al.~\cite{wurzer2015kterm} except they use the proportion of new such sets to quantify the novelty of incoming articles while our approach focuses on the new sets themselves and the terms within them. However, the underlying principle is the same: the first report of Prince's death is likely to be the first document within the stream containing the words ``Prince'' and ``dead'' or similar. 

In more formal terms, Damaschke~\cite{damaschke2015pairs} presents the definition of \emph{minimal new sets}. 

\begin{definition}
  Let $B_0, B_1, B_2...B_{m-1}$ be a sequence of sets that we call bags. For another bag $B:=B_{m}$ we call a subset $X \subseteq B$ new at $m$, if $X$ was not already a subset of an earlier bag : $\forall i < m : X \setminus B_{i} \neq \emptyset$. Otherwise $X$ is said to be old at $m$. We call $X \subseteq B$ minimal new at $m$ if $X$ is new and also minimal (with respect to inclusion) with this property.
\end{definition}

Thus, to continue with our previous example, if we tokenize incoming news articles into bags of words, then $\{prince, dead\}$ would likely be a minimal new set for the first article reporting the tragic event of Prince's death.

A first, naive approach could be to simply base the detection of new events on the existence of minimal sets below a given set size $k$. For example, if we choose $k=3$ we would say that if an article contains new words or new pairs of old words it contains a new event, otherwise it does not. This naive approach does, however, not work well in practice since articles covering the same event are quite likely to contain new words or new pairs and thus this method is likely to falsely flag articles as containing new events.

Instead of blindly flagging articles based on the existence of minimal sets we propose a method which automatically highlights and presents important informative word combinations by certain quantitative criteria such that the user can then quickly browse them and see which of them raise interest. This introduces the problem of having to prioritize the minimal sets identified for any incoming article.

\section{Enumeration}
\label{method:enumeration}
Central to our approach is the process of enumerating minimal new sets from incoming articles. Luckily, Damaschke~\cite{damaschke2015pairs} presents an effective enumeration algorithm:

Let's say that at time $m$ we have stored the bags of previous articles sequentially: $B_0, B_1, B_2...B_{m-1}$. In order to find all minimal new sets of bag $B_{m}$ we generate candidate sets $X \subset B_{m}$ of increasing sizes until all candidates are supersets of already discovered minimal new sets at $m$. First we find all, if any, words that are new at $m$, then from any words that are not new at $m$ we find all possible sets of size 2 (pairs) and check for each of them if they are new at $m$. If there are any pairs that were not new at $m$ we find sets of size 3 from those, etc. 

The key to making the algorithm efficient lies in how we determine whether a set $X$ is new at $m$. We create a function $f$, so that $f(X) := min\{i \mid X \subseteq B_{i}\}$ and let $f(X)$ be undefined if $X$ is not a subset of any bag $B_{i}$. Now $X$ is new at $f(X)=i$ if such a value exists, and old at any subsequent index $j>i$. 

When enumerating minimal new sets of bag $B_{i}$ we store $X$ along with $f(X)=i$ for each of the candidate sets that we determine to be minimal new at $i$. For any set of size 1 we can easily determine if it is a minimal new set simply by checking whether a value for that set has been stored in the table.  Any time that a word makes it first appearance any larger subset of words from the same bag containing that word will also be new at that time, but not minimal. For any larger candidate set $X \subseteq B_{m}$, $|X|>1$ we therefore might have to take further steps to determine whether $X$ is new at $m$. Again, if $f(X)$ is stored we know that $X$ is old. If $f(X)$ is not stored then for each $Y \subset X$ we check if $f(Y)$ is stored. If $f(Y)=j$ is stored then we check if $X \subset B_{j}$. If such a $j$ is found then $X$ was new at $j$ but is old at $m$. If no such $m$ is found we conclude that $X$ is new at $m$. The minimal new sets of bag $B_{j}$ can be enumerated in \BigO{|B_{n}|2^{k}/k!}.


\section{Pre-processing}
\label{method:preprocessing}
In order to enumerate minimal sets from an article, some preprocessing is required. Incoming articles are tokenized, i.e. split into lists of words, stopwords, the most common words of the input language, are removed along with any punctuation and the remaining words are stemmed using a Porter2~\cite{porter2002english} stemmer and converted to lowercase.

\section{Extensions}
As explained in section~\ref{section:idea} we might want to improve the performance of our results by limiting the outputs of the enumerations. Two simple ways of doing this are to simply limit the input the enumeration algorithm receives or filtering its output.
In this section we will discuss the approaches we examined in order to try to accomplish this which mostly are based on filtering common or unimportant words, prioritizing words that occur in many minimal new pairs and prioritize pairs of words that appear with close proximity.

\subsection{Filtering common or unimportant words}
Words that describe a new event are likely to be relatively unique to that event. Reversely, words which are common over multiple different articles are unlikely to be descriptive for new specific events. Similarly, words which are common within an article are likely to be important for that article. We could filter out words that are common within the previously seen articles, or words which can be determined not to be important within the incoming article. We will examine filters based on three different metrics
\begin{itemize}
  \item \emph{collection frequency}: how often a given word has appeared within the accumulated corpus.
  \item \emph{document frequency}: the number of documents in which a given word has previously appeared.
  \item \emph{TF-IDF} score: a score indicating the importance of a word within the given article with relation to how common it is within the corpus. TF-IDF is calculated as $tfidf = tf*idf$ where $tf$ is the term frequency of the given word within the given article and $idf = \log{\frac{N}{df}}$ for $N$ as the number of processed articles and $df$ the document frequency of the given word.
\end{itemize}

This type of filtering can be done in several different ways. We could define thresholds for our filters and focus on words which score above or below that threshold, this is similar to the approach used by Brants et al.~\cite{brants2003system} where any words with a document frequency below 2 was discarded. However, determining the thresholds might prove difficult and we most certainly do not want to discard all words which have not appeared in previous bags. Another approach would be to select the $n$ words with the highest or lowest score and focus on them. In either case we also have the option to either completely ignore all words but the ones we have selected or to choose only sets containing our chosen words. The distinction between these two approaches is subtle but important. Given that the two words ``Prince'' and ``purple'' are part of the same bag but only ``Prince'' satisfies the condition of our filter. If the pair (``Prince'', ``purple'') is minimal new in the unfiltered bag the first approach would not output it since it contains ``purple'' which is to be removed, while the second approach would output it since the pair contains ``Prince'' which is to be included. 

Another important aspect to consider is when filtering is applied. If filtering is done as part of the preprocessing step, then the pair in the above example will not be found. Consequently, any subsets containing the filtered words will not be stored in the table of $f(X)$ values. In that case only the filtered bags should be stored and filtered words are free to be identified as new in later bags. 

\subsection{Prioritizing words that occur in many minimal new pairs}
It is not hard to imagine that an article reporting a new event is likely to contain many new pairs of words that previously have not appeared together. Commonly used words are however likely to have already appeared together. Thus, minimal new sets of size $>1$ are likely to contain at least one word that is more indicative of the events described within the given article. If a particular word can be found within a majority of the minimal new sets, then it is likely to be an old word within a new context and likely to be a key property of the article. For instance, the words ``purple'', ``rain'' and ``Prince'' are likely to have appeared together before in Prince related articles. An article that then yields the minimal new sets $\{Prince, dead\}, \{purple, dead\}, \{rain, dead\}$ gives us a  common denominator in ``dead'' within the minimal set, indicating that ``dead'' is likely to be an important indicator of a possible new event. Based on this idea we can identify important words by counting the number of minimum new sets they appear in. In particular we focus on prioritizing words that occur in many minimal new pairs.

\subsection{Prioritizing words that appear in close proximity}
Words that are indicative of new events are likely to be found in close proximity. For instance an article reporting about a volcanic eruption at Yellowstone is much more likely to contain something along the lines of ``Volcanic eruption at Yellowstone'' than mentioning ``Yellowstone'' in one paragraph and ``eruption'' several paragraphs later.

For any minimal new sets of an article we can calculate the minimum amount of words between the elements within the sets and find which sets contain words which occur close to each other within the article. Using this we can prioritize sets of words which appear more closely together within the article in the hope that they are good indicators of potential events.

\subsection{Splitting articles into smaller parts}
Another approach for increasing the importance of words that appear together is to split an article into several smaller, sub-articles. Some natural examples of such sub-articles would be paragraphs or sentences. One can easily imagine that important event-related words appear within the same paragraph or even the same sentence. This requires an extra pre-processing step: splitting up incoming articles into the wanted bits. In addition this requires some trivial changes to the base algorithm. For each article we have to feed each sub-bag to our algorithm but take care not to add any words to the list of previously seen bags until after we have processed all the sub-bags of the incoming article so as to avoid identifying words that first appear within the given article as old in sub-bags which are subsequent to the sub-bag in which they first appear.

Let the sub-bags $P_{k} := [S_{0}, S_{1},...,S_{q}]$ be a list of the sub-bags generated from article $k$ so that for the bag of all the words in the article $B_{k}$ it holds that $\bigcup_{i=0}^{q}S_{i} = B_{k}$. We call $P_{k}$ the parent bag for article $k$. We assume that we have stored all the previous parent bags $S_{0},...,S_{k-1}$ as well as mappings from all previously minimal bags to sub-bags within parent-bags. The enumeration of $B_{k}$ is now the union of all the enumerations from the sub-bags $S_{i} \mid i \in [0,q]$ where the minimal news sets of each $S_{i}$ are enumerated using the approach described in section~\ref{method:enumeration}. If a minimal new set $X$ is found while processing sub-bag $S_{i} \in P_{k}$ we store the minimal new set along with both the numbers $k$ and $i$. If the same set $X$ is found in another sub-bag $S_{j} \in P_{k}$ where $j>i$ we need to make sure to store $j$ as well. We let $f'(X)$ be a function which returns the number of the  parent-bag in which $X$ was minimal new as the sub-bags of that bag which contained $X$. In other words we store $f'(X) = (k, [i,j])$, to indicate that $X$ was minimal new at $k$ and found in sub-bags $i$ and $j$. 

Just like in the original enumeration algorithm, we don't have to rely on naive exhaustive search to calculate $f'(X)$ for any $|X|>1$. For each $Y\subset X$ we can lookup $f'(Y)$ and if a pair $(k, [l_{0}... l_{q}])$ is found we check $X \subseteq S_{k, l_{i}} \mid S_{k, l_{i}} \in P_{k}, i\in[0,q]$. If such a $S_{k,l_{i}}$ is found we know that $X$ is old. Furthermore we can conclude that $X$ was new at the smallest $k \mid f'(Y)=(k, [l_{0},..., l_{q}])$.

\begin{theorem}
  Given that we have previously processed bags $B_{1}...B_{n-1}$ the minimal-new sets of maximum size $h$ can be enumerated from sub-bag $S_{n, j}\in B_{n}$ in \BigO{p|S_{j}|2^{h}/h!}, where $p$ is the amount of sub-bags in any bag $B_{i} \mid i \in \mathbb{N}, i \in [1,n-1]$.
\end{theorem}
\begin{proof}
  For any given candidate set $X \subseteq S_{n, j}$ of size $r$ containing only words which we have previously seen we know that if $f'(X)$ is undefined we have to look up $f'(Y)$ for each $Y \subset X$. For $f'(Y) = (k, l_{0},..,l_{q})$ we then check if $Y \subseteq S_{k, t} \mid t \in f'(Y)$. We will have to lookup at most $2^{r}-2$ candidate sets and check at most $p$ possible subsets $S_{k, t}$. So we can check if a set $X$ of size $r$ is new in \BigO{p2^r}. The number of candidate sets we have to consider is $\binom{|S_{n, j}|}{r} < \frac{|B_{n}|}{r!}$. It follows that finding all minimal new sets of size $h$ is bounded by \BigO{p|S_{j}|2^{h}/h!}.
\end{proof}

\subsubsection{Joining sub-bags}
Using the same idea we can create a ``sliding window'' of $k$ sub-bags to increase the sizes of the sub-bags and thus increasing the odds of finding new combinations of words. A simple example would be to always use three consecutive sentences as sub-bags when available. Thus the first three sentences of an article would make up a sub-bag, then the second to the fourth sentence would make up the next sub-bag etc. For each consecutive sentence the first sentence of the previous bag would be removed and the new sentence added, like sliding a three-sentence wide window over the article. This approach allows us to explore larger sub-bags while preventing us from missing important word combinations which we could miss if we were to simply choose the first three sentences followed by the next three etc. Using a similar approach as previously explained for sub-bags we store the number of the parent bag along with the number of the sub-bag. The only difference now is that the sub-bags are slightly larger and fewer.

\subsection{Combining methods}
We can of course also combine several of the methods described above. For instance we could generate sub-bags from incoming articles but filter the output of the enumeration to only include subsets with words that scored above a given TF-IDF threshold.

\chapter{Method}
\label{chapter:method}
On-line new event detection is hard~\cite{allan2000hard}. In this chapter we present a new method for ONED based around the concept of \emph{minimal new pairs}. We explain the rationale behind the idea, give a detailed explanation of the base algorithm and discuss some of the approaches we have examined for expanding upon the base idea.


\section{The basic idea}
\label{section:idea}
Damaschke~\cite{damaschke2015pairs} observed that for new articles, the existence of previously unseen small sets of words could be good indicators for new events and presented an efficient algorithm for finding such sets. This is similar to the approach used by Wurzer et al.~\cite{wurzer2015kterm} except they use the proportion of new such sets to quantify the novelty of the novelty of incoming articles while our approach focuses on the new sets themselves and the terms within them. However, the base principle is the same: the first report of Prince's death is likely to be the first document within the stream containing the words ``Prince'' and ``dead'' or similar. 

In more formal terms, Damaschke~\cite{damaschke2015pairs} presents the definition of \emph{minimal new sets}. 

\begin{definition}
  Let $B_0, B_1, B_2...B_{m-1}$ be a sequence of sets that we call bags. For another bag $B:=B_{m}$ we call a subset $X \subseteq B$ new at $m$, if $X$ was not already a subset of an earlier bag : $\forall i < m : X \setminus B_{i} \neq \emptyset$. Otherwise $X$ is said to be old at $m$. We call $X \subseteq B$ minimal new at $m$ if $X$ is new and also minimal (with respect to inclusion) with this property.
\end{definition}

Thus, to continue with our previous example, if we tokenize incoming news articles into bags of words, then $\{prince, dead\}$ would likely be a minimal new set for the first article reporting the tragic event of Prince's death.

A first, naive approach could be to simply base the detection of new events on the existence of minimal sets below a given set size $k$. For example, if we choose $k=3$ we would say that if an article contains new words or new pairs of old words it contains an new event, otherwise it does not. This naive approach does not work well in practice however since articles covering the same event are quite likely to contain new words or new pairs and thus this method is likely to falsely flag articles as containing new events.

Instead of blindly flagging articles based on the existence of minimal sets we propose a method which automatically highlights and presents important informative word combinations by certain quantitative criteria such that the user can then quickly browse them and see which of them raise interest. This introduces the problem of having to prioritize the minimal sets identified for any incoming article.

\section{Enumeration}
TODO: explain Damaschke's enumeration algorithm as the basis for our method.

\section{Pre-processing}
Prior to extracting the minimal sets from an article the article must be preprocessed so that we can digest it. For our enumeration algorithm [TODO: ``enumeration'' is explained in Damaschke2015 but we must also explain it here before we talk about it] we treat articles as sets of words, \emph{bags}, and therefore we must pre-process any article that we receive as input before we can enumerate it and apply any of our methods. We tokenize the input articles, remove stopwords and punctuation and stem the tokenized words using the English porter2 stemmer provided by the NLTK. [TODO: add citation for NLTK and porter2]

\section{Approaches}
As explained in section~\ref{section:idea} we have to filter out or select from the minimal sets that we identify within incoming articles in order to be able to present the informative sets that are likely to be indicative of a new event. In this section we will discuss the approaches we examined in order to try to accomplish this: filtering common or unimportant words, prioritizing words that occur in many minimal new pairs, prioritize words that appear with close proximity [TODO: more approaches, more details and complexity analysis].

\subsection{Filtering common or unimportant words}
Words which describe a new event are likely to be relatively unique to that event. Reversely, words which are common over multiple different articles are likely to not be descriptive for new specific events. [TODO: citation?] Similarly, words which are common within an article are likely to be important for that article. Thus, we suggest filtering out words that are common within the previously seen articles, or words which can be determined not to be important within the incoming article. Thus we can filter based on \emph{collection frequency}: how often a given word has appeared within the accumulated corpus; \emph{document frequency}: the number of documents in which a given word has previously appeared; the \emph{TF-IDF} score: a score indicating the importance of a word within the given article with relation to how common it is within the corpus.

\subsection{Prioritizing words that occur in many minimal new pairs}
An article reporting a new event is likely to contain many new pairs of words that previously have not appeared together. [TODO: citation?] Commonly used words are however likely to have appeared together. Thus, minimal new sets of size $>1$ are likely to contain at least one word that is more indicative of the events described within the given article. If a particular word can be found within a majority of the minimal new sets, then it is likely to be an old word within a new context and likely to be a key property of the article. For instance, the words ``purple'', ``rain'' and ``Prince'' are likely to have appeared together before in Prince related articles. An article that then yields the minimal new sets $\{Prince, dead\}, \{purple, dead\}, \{rain, dead\}$ gives us a  common denominator in ``dead'' within the minimal set, indicating that ``dead'' is likely to be an important indicator of a possible new event. Based on this idea we can identify important words by counting the number of minimum new sets they appear in. In particular we focus on prioritizing words that occur in many minimal new pairs.

\subsection{Prioritizing words that appear in close proximity}
Words that are indicative of new events are likely to be find in close proximity. For instance an article reporting about a volcanic eruption at Yellowstone is much more likely to contain something along the lines of ``Volcanic eruption at Yellowstone'' than mentioning ``Yellowstone'' in one paragraph and ``eruption'' several paragraphs later.

For any minimal new sets of an article we can calculate the minimum amount of words between the elements within the sets and find which sets contain words which occur close to each other within the article. Using this we can prioritize sets of words which appear more closely together within the article in the hopes that they are good indicators of potential events.

\subsection{Splitting articles into smaller parts}
Another approach for increasing the importance of words that appear together is to split an article into several smaller, sub-articles. Some natural examples of such sub-articles would be paragraphs or either sentences. One can easily imagine that important event-related words appear within the same paragraph or even the same sentence. This requires an extra pre-processing step: splitting up incoming articles into the wanted bits. In addition this requires some trivial changes to the base algorithm. For each article we have to feed each sub-bag to our algorithm but take care not to add any words to the list of previously seen bags until after we have processed all the sub-bags of the incoming article so as to avoid identifying words that first appear within the given article as old in sub-bags which are subsequent to the sub-bag in which they first appear.

\subsubsection{Joining sub-bags}
Going off the same idea we can create a ``sliding window'' of $n$ sub-bags to increase the sizes of the sub-bags and thus increasing the odds of finding new combinations of words. A simple example would be to always use three consecutive sentences as sub-bags when available. Thus the first three sentences of an article would make up a sub-bag, then the second to the fourth sentence would make up the next sub-bag etc. For each consecutive sentence the first sentence of the previous bag would be removed and the new sentence added, like sliding a three-sentence wide window over the article. This approach allows us to explore larger sub-bags while preventing us from missing important word combinations which we could miss if we were to simply choose the first three sentences followed by the next three etc.

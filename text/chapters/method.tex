\chapter{Method}
\label{chapter:method}
On-line new event detection is hard~\cite{allan2000hard}. In this chapter we present a new method for ONED based around the concept of \emph{minimal new pairs}. We explain the rationale behind the idea, give a detailed explanation of the base algorithm and discuss some of the approaches we have examined for expanding upon the base idea.


\section{The basic idea}
\label{section:idea}
Damaschke~\cite{damaschke2015pairs} observed that for new articles, the existence of previously unseen small sets of words could be good indicators for new events and presented an efficient algorithm for finding such sets. This is similar to the approach used by Wurzer et al.~\cite{wurzer2015kterm} except they use the proportion of new such sets to quantify the novelty of the novelty of incoming articles while our approach focuses on the new sets themselves and the terms within them. However, the base principle is the same: the first report of Prince's death is likely to be the first document within the stream containing the words ``Prince'' and ``dead'' or similar. 

In more formal terms, Damaschke~\cite{damaschke2015pairs} presents the definition of \emph{minimal new sets}. 

\begin{definition}
  Let $B_0, B_1, B_2...B_{m-1}$ be a sequence of sets that we call bags. For another bag $B:=B_{m}$ we call a subset $X \subseteq B$ new at $m$, if $X$ was not already a subset of an earlier bag : $\forall i < m : X \setminus B_{i} \neq \emptyset$. Otherwise $X$ is said to be old at $m$. We call $X \subseteq B$ minimal new at $m$ if $X$ is new and also minimal (with respect to inclusion) with this property.
\end{definition}

Thus, to continue with our previous example, if we tokenize incoming news articles into bags of words, then $\{prince, dead\}$ would likely be a minimal new set for the first article reporting the tragic event of Prince's death.

A first, naive approach could be to simply base the detection of new events on the existence of minimal sets below a given set size $k$. For example, if we choose $k=3$ we would say that if an article contains new words or new pairs of old words it contains a new event, otherwise it does not. This naive approach does not work well in practice however since articles covering the same event are quite likely to contain new words or new pairs and thus this method is likely to falsely flag articles as containing new events.

Instead of blindly flagging articles based on the existence of minimal sets we propose a method which automatically highlights and presents important informative word combinations by certain quantitative criteria such that the user can then quickly browse them and see which of them raise interest. This introduces the problem of having to prioritize the minimal sets identified for any incoming article.

\section{Enumeration}
\label{method:enumeration}
Central to our approach is the enumeration of incoming articles into, i.e. identifying the minimal new sets. Luckily, Damaschke~\cite{damaschke2015pairs} presents an effective way to enumerate articles:

%In the first pass we look for new words by looking up each word in the bag in some sort of efficient datastructure such as a hashmap. If the word is not found during lookup we store it along with the number of the article that we are currently processing. We also conclude that the word is new. If the word is found during lookup we know that we have encountered it before. From the value stored along with it we also know when it was first encountered. 
%Once we have processed all the words in the bag we potentially have a set of new words which we can compare with the bag. We remove these before the second pass in which we construct all possible sets containing two distinct words from the words that are left in the bag. Next we lookup each of the pairs we've identified in the same manner as before. However, if lookup comes up empty we now have to check that the words in the pair didn't appear together in an article when at least one of them appeared for the first time. Thus, for each of the words in the set we perform a lookup and if the words is found we check if our pair is a subset of the bag in which that word was first encountered. If we can find a bag where the pair is a subset then the pair is not new in the current input article. If no such bag can be found however, then we can conclude that the pair is a minimal new subset of size 2. 
%We now remove all pairs we have identified as new from the bag of pairs and recognize this as the end of the second pass.
%If there are any pairs left we find all subsets of size 3 from the bag of pairs and proceed in the same way until there are the bag is empty at the end of a pass. 

Let's say that at time $m$ we have stored the bags of previous articles sequentially: $B_0, B_1, B_2...B_{m-1}$. In order to find all minimal new sets of bag $B_{m}$ we generate candidate sets $X \subset B_{m}$ of increasing sizes until all candidates are supersets of already discovered minimal new sets. First we find all, if any, words that are new at $m$, then from any words that are not new at $m$ we find all possible sets of size 2 (pairs) and check for each of them if they are new at $m$. If there are any pairs that were not new at $m$ we find sets of size 3 from those, etc. 

The key to making the algorithm efficient lies in how we determine whether a set $X$ is new at $m$. We create a function, $f$ so that $f(X) := min\{i \mid X \subseteq B_{i}\}$ and let $f(X)$ be undefined if $X$ is not a subset of any bag $B_{i}$. Now $X$ is new at $f(X)=i$ if such a value exists, and old at any subsequent index $j>i$. 

When enumerating bag $B_{i}$ we store $X$ along with $f(X)=i$ for each of the candidate sets that we determine to be minimal new at $i$. For any set of size 1 we can easily determine if it is a minimal new set simply by checking whether a value for that set has been stored in the table.  Any time that a word makes it first appearance any larger subset of words from the same bag containing that word will also be new at that time, but not minimal. For any larger candidate set $X \subseteq B_{m}$, $|X|>1$ we therefore might have to take further steps to determine whether $X$ is new at $m$. Again, if $f(X)$ is stored we know that $X$ is old. If $f(X)$ is not stored then for each $Y \subset X$ we check if $f(Y)$ is stored. If $f(Y)=j$ is stored then we check if $X \subset B_{j}$. If such a $j$ is found then $X$ was new at $j$ but is old at $m$. If no such $m$ is found we conclude that $X$ is new at $m$.


\section{Pre-processing}
Prior to extracting the minimal sets from an article the article must be preprocessed so that we can digest it. For our enumeration algorithm [TODO: ``enumeration'' is explained in Damaschke2015 but we must also explain it here before we talk about it] we treat articles as sets of words, \emph{bags}, and therefore we must pre-process any article that we receive as input before we can enumerate it and apply any of our methods. We tokenize the input articles, remove stopwords and punctuation and stem the tokenized words using the English porter2 stemmer provided by the NLTK. [TODO: add citation for NLTK and porter2]

\section{Approaches}
As explained in section~\ref{section:idea} we have to filter out or select from the minimal sets that we identify within incoming articles in order to be able to present the informative sets that are likely to be indicative of a new event. In this section we will discuss the approaches we examined in order to try to accomplish this: filtering common or unimportant words, prioritizing words that occur in many minimal new pairs, prioritize words that appear with close proximity.

\subsection{Filtering common or unimportant words}
Words which describe a new event are likely to be relatively unique to that event. Reversely, words which are common over multiple different articles are likely to not be descriptive for new specific events. [TODO: citation?] Similarly, words which are common within an article are likely to be important for that article. Thus, we suggest filtering out words that are common within the previously seen articles, or words which can be determined not to be important within the incoming article. Thus we can filter based on \emph{collection frequency}: how often a given word has appeared within the accumulated corpus; \emph{document frequency}: the number of documents in which a given word has previously appeared; the \emph{TF-IDF} score: a score indicating the importance of a word within the given article with relation to how common it is within the corpus.

\subsection{Prioritizing words that occur in many minimal new pairs}
An article reporting a new event is likely to contain many new pairs of words that previously have not appeared together. [TODO: citation?] Commonly used words are however likely to have appeared together. Thus, minimal new sets of size $>1$ are likely to contain at least one word that is more indicative of the events described within the given article. If a particular word can be found within a majority of the minimal new sets, then it is likely to be an old word within a new context and likely to be a key property of the article. For instance, the words ``purple'', ``rain'' and ``Prince'' are likely to have appeared together before in Prince related articles. An article that then yields the minimal new sets $\{Prince, dead\}, \{purple, dead\}, \{rain, dead\}$ gives us a  common denominator in ``dead'' within the minimal set, indicating that ``dead'' is likely to be an important indicator of a possible new event. Based on this idea we can identify important words by counting the number of minimum new sets they appear in. In particular we focus on prioritizing words that occur in many minimal new pairs.

\subsection{Prioritizing words that appear in close proximity}
Words that are indicative of new events are likely to be find in close proximity. For instance an article reporting about a volcanic eruption at Yellowstone is much more likely to contain something along the lines of ``Volcanic eruption at Yellowstone'' than mentioning ``Yellowstone'' in one paragraph and ``eruption'' several paragraphs later.

For any minimal new sets of an article we can calculate the minimum amount of words between the elements within the sets and find which sets contain words which occur close to each other within the article. Using this we can prioritize sets of words which appear more closely together within the article in the hopes that they are good indicators of potential events.

\subsection{Splitting articles into smaller parts}
Another approach for increasing the importance of words that appear together is to split an article into several smaller, sub-articles. Some natural examples of such sub-articles would be paragraphs or either sentences. One can easily imagine that important event-related words appear within the same paragraph or even the same sentence. This requires an extra pre-processing step: splitting up incoming articles into the wanted bits. In addition this requires some trivial changes to the base algorithm. For each article we have to feed each sub-bag to our algorithm but take care not to add any words to the list of previously seen bags until after we have processed all the sub-bags of the incoming article so as to avoid identifying words that first appear within the given article as old in sub-bags which are subsequent to the sub-bag in which they first appear.

Let the sub-bags $P_{k} := [S_{0}, S_{1},...,S_{q}]$ be a list of the sub-bags generated from article $k$ so that for the bag of all the words in the article $B_{k}$ it holds that $\bigcup_{i=0}^{q}S_{i} = B_{k}$. We call $P_{k}$ the parent bag for article $k$. We assume that we have stored all the previous parent bags $S_{0},...,S_{k-1}$ as well as mappings from all previously minimal bags to sub-bags within parent-bags. The enumeration of $B_{k}$ is now the union of the all the enumerations of the sub-bags $S_{i} \mid i \in [0,q]$ where each $S_{i}$ is enumerated using the approach described in section~\ref{method:enumeration}. If a minimal new set $X$ is found while processing sub-bag $S_{i} \in P_{k}$ we store the minimal new set along with both the numbers $k$ and $i$. If the same set $X$ is found in another sub-bag $S_{j} \in P_{k}$ where $j>i$ we need to make sure to store $j$ as well. We let $f'(X)$ be a function which returns the number of the  parent-bag in which $X$ was minimal new as the sub-bags of that bag which contained $X$. In other words we store $f'(X) = (k, [i,j])$, to indicate that $X$ was minimal new at $k$ and found in sub-bags $i$ and $j$. 

Just like in the original enumeration algorithm, we don't have to rely on naive exhaustive search to calculate $f'(X)$ for any $|X|>1$. For each $Y\subset X$ we can lookup $f'(Y)$ and if a pair $(k, [l_{0}... l_{q}])$ is found we check $X \subseteq S_{k, i} \mid S_{k, i} \in P_{k}, i\in[0,q]$. If such a $S_{k,i}$ is found we know that $X$ is old. Furthermore we can conclude that $X$ was new at the smallest $k \mid f'(Y)=(k, [l_{0},..., l_{q}])$.

\subsubsection{Joining sub-bags}
Using the same idea we can create a ``sliding window'' of $k$ sub-bags to increase the sizes of the sub-bags and thus increasing the odds of finding new combinations of words. A simple example would be to always use three consecutive sentences as sub-bags when available. Thus the first three sentences of an article would make up a sub-bag, then the second to the fourth sentence would make up the next sub-bag etc. For each consecutive sentence the first sentence of the previous bag would be removed and the new sentence added, like sliding a three-sentence wide window over the article. This approach allows us to explore larger sub-bags while preventing us from missing important word combinations which we could miss if we were to simply choose the first three sentences followed by the next three etc.
